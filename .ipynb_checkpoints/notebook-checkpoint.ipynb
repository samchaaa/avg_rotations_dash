{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('./download-tick-from-dukascopy'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import download_tick_from_dukascopy\n",
    "\n",
    "path = 'data'\n",
    "inst_txt_path = 'insts.txt'\n",
    "\n",
    "DATE_RANGE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inst = 'BTCUSD'\n",
    "# inst = 'USA500IDXUSD'\n",
    "# inst = 'USATECHIDXUSD'\n",
    "\n",
    "# if inst not in os.listdir(path):\n",
    "#     os.mkdir('{}/{}'.format(path, inst))\n",
    "    \n",
    "#\n",
    "#   To do:\n",
    "#       - Check if already in data\n",
    "#       - Download days you don't have\n",
    "#\n",
    "    \n",
    "# gotta loop through this, and handle no columns (1kb .csv)\n",
    "\n",
    "# check handling quitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(inst):\n",
    "    # concat all .csvs within dir\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    files = [\n",
    "        file for file \n",
    "        in os.listdir('{}/{}'.format(path, inst)) \n",
    "        if os.path.getsize('{}/{}/{}'.format(path, inst, file)) > 2\n",
    "    ]\n",
    "    \n",
    "    for csv in files:\n",
    "        data = pd.concat([\n",
    "            data,\n",
    "            pd.read_csv(\n",
    "                '{}/{}/{}'.format(path, inst, csv),\n",
    "                header=None\n",
    "            )\n",
    "        ])\n",
    "    data.columns = ['dt', 'bid', 'ask', 'bidvol', 'askvol']\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean(inst):\n",
    "    data = get_all(inst)\n",
    "    # dd = len(data)\n",
    "    data = data.drop_duplicates()\n",
    "    # print('{} duplicate rows dropped'.format(dd-len(data)))\n",
    "    # data = data.sort_values('dt')\n",
    "    # print('max: {}'.format(data['dt'].max()))\n",
    "    data['mid'] = (data['bid'] + data['ask']) / 2\n",
    "    data.index = data['dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "    data = data[['mid']].resample('1Min').ohlc()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inst = 'USATECHIDXUSD'\n",
    "# # inst = 'BTCUSD'\n",
    "# inst = 'USA500IDXUSD'\n",
    "\n",
    "# data = clean(inst)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take last datetime, find difference from current date\n",
    "# # datetime.strptime(data.index.max(), '%Y-%m-%d %H:%M:%S.%f')\n",
    "# (datetime.now() - data.index.max()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just get last week because it's less annoying\n",
    "# datetime.strftime((datetime.now() - timedelta(7)), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class insts list\n",
    "\n",
    "# class inst\n",
    "    # start date\n",
    "    # end date\n",
    "    # write to csv\n",
    "    # do things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(inst_txt_path, 'r') as f:\n",
    "#     insts = f.read().split('\\n')\n",
    "# print(insts)\n",
    "\n",
    "# for inst in insts:\n",
    "#     print(inst)\n",
    "#     if inst not in os.listdir(path):\n",
    "#         os.mkdir('{}/{}'.format(path, inst))\n",
    "#     print(os.listdir('{}/{}'.format(path, inst)))\n",
    "    \n",
    "#     # consider making some meta-file that keeps last updated date instead of concatting everytime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_max(inst):\n",
    "    \n",
    "    # Returns min/max date from current data by:\n",
    "    # 1.) Finding max filename (doesn't account for end date of data .csv)\n",
    "    # 2.) Finding max date within that file (doesn't account for gaps)\n",
    "    \n",
    "    files = [\n",
    "        file for file \n",
    "        in os.listdir('{}/{}'.format(path, inst)) \n",
    "        if os.path.getsize('{}/{}/{}'.format(path, inst, file)) > 2\n",
    "    ]\n",
    "    \n",
    "    if files:\n",
    "    \n",
    "        # Check\n",
    "        csv = max(files)\n",
    "        inst_max = re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', max(files)).group(0)\n",
    "\n",
    "        #     data = pd.read_csv('{}/{}/{}'.format(path, inst, csv), header=None)\n",
    "        #     data.columns = ['dt', 'bid', 'ask', 'bidvol', 'askvol']\n",
    "        #     # data = data.drop_duplicates()\n",
    "        #     data.index = data['dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "        #     # data = data[['bid']].resample('1Min').first()\n",
    "        #     inst_max = data.index.max().date()\n",
    "\n",
    "        # Get min date data\n",
    "        csv = min(files)\n",
    "        inst_min = re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', min(files)).group(0)\n",
    "\n",
    "        #     data = pd.read_csv('{}/{}/{}'.format(path, inst, csv), header=None)\n",
    "        #     data.columns = ['dt', 'bid', 'ask', 'bidvol', 'askvol']\n",
    "        #     data.index = data['dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "        #     inst_max = data.index.max().date()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # inst_max = datetime.strftime(datetime.today(), '%Y-%m-%d')\n",
    "        # inst_min = datetime.strftime((datetime.today() - BDay(DATE_RANGE)).date(), '%Y-%m-%d')\n",
    "        inst_min = False\n",
    "        inst_max = False\n",
    "    \n",
    "    return inst_min, inst_max\n",
    "\n",
    "\n",
    "def download(inst, start_date, end_date):\n",
    "    \n",
    "    # given start and end date, download each inst a day at a time\n",
    "    \n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    diff = (end_dt - start_dt).days\n",
    "    date_list = [datetime.strftime(end_dt - timedelta(days=x), '%Y-%m-%d') for x in range(0, diff)]\n",
    "    \n",
    "    for date in date_list:\n",
    "        download_tick_from_dukascopy.main(inst, date, date, './{}/{}'.format(path, inst))\n",
    "\n",
    "        \n",
    "def check_current(inst):\n",
    "    \n",
    "    # check current date against last updated date for all inst's\n",
    "    \n",
    "    inst_min, inst_max = check_max(inst)\n",
    "\n",
    "    if all([inst_min, inst_max]):\n",
    "    \n",
    "        # Checking start date\n",
    "        # If min < than this, you're good.\n",
    "        if datetime.strptime(inst_min, '%Y-%m-%d').date() < (datetime.today() - BDay(DATE_RANGE)).date():\n",
    "            print('start date good')\n",
    "\n",
    "        # Else, download from this to whereever the min was.  \n",
    "        else:\n",
    "            to_download_start = datetime.strftime((datetime.today() - BDay(DATE_RANGE)).date(), '%Y-%m-%d')\n",
    "            to_download_end = inst_min\n",
    "\n",
    "            download(inst, to_download_start, to_download_end)\n",
    "\n",
    "        # Checking end date\n",
    "        if datetime.strptime(inst_max, '%Y-%m-%d').date() == datetime.today().date():\n",
    "            print('end date good')\n",
    "\n",
    "        # else, download from max to today\n",
    "        else:\n",
    "            to_download_start = inst_max\n",
    "            to_download_end = datetime.strftime(datetime.today(), '%Y-%m-%d')\n",
    "\n",
    "            download(inst, to_download_start, to_download_end)\n",
    "            \n",
    "    else:\n",
    "        to_download_start = datetime.strftime((datetime.today() - BDay(DATE_RANGE)).date(), '%Y-%m-%d')\n",
    "        to_download_end = datetime.strftime(datetime.today(), '%Y-%m-%d')\n",
    "\n",
    "        download(inst, to_download_start, to_download_end)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def handle_check(inst):\n",
    "    \n",
    "    tries = 0\n",
    "    \n",
    "    try:\n",
    "        check_current(inst)\n",
    "    except OSError as e: # may be redundant if handling in update()\n",
    "        print(e)\n",
    "        \n",
    "        if inst not in os.listdir(path):\n",
    "            os.mkdir('{}/{}'.format(path, inst))\n",
    "        \n",
    "        handle_check(inst)\n",
    "    except urllib2.URLError as e: # not sure if this wil work, will have to check later... could make inf loop\n",
    "        print('URLError', e)\n",
    "        tries += handle_check(inst)\n",
    "        print('e tries', tries)\n",
    "        \n",
    "    print('all tries', tries)\n",
    "    return tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    # update all things\n",
    "        \n",
    "    with open(inst_txt_path, 'r') as f:\n",
    "        insts = f.read().split('\\n')\n",
    "    print(insts)\n",
    "\n",
    "    for inst in insts:\n",
    "        print(inst)\n",
    "        if inst not in os.listdir(path):\n",
    "            os.mkdir('{}/{}'.format(path, inst))\n",
    "        # print(os.listdir('{}/{}'.format(path, inst)))\n",
    "        \n",
    "        handle_check(inst)\n",
    "    \n",
    "    # consider making some meta-file that keeps last updated date instead of concatting everytime\n",
    "    \n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inst = 'USATECHIDXUSD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GBPUSD', 'BTCUSD', 'USA500IDXUSD', 'USATECHIDXUSD']\n",
      "GBPUSD\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/00h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/01h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/02h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/03h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/04h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/05h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/06h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/07h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/08h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/09h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/10h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/11h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/12h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/13h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/14h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/15h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/16h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/17h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/18h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/19h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/20h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/21h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/22h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/07/23h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/00h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/01h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/02h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/03h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/04h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/05h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/06h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/07h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/08h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/09h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/10h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/11h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/12h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/13h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/14h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/15h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/16h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/17h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/18h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/19h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/20h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/21h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/22h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/GBPUSD/2021/03/06/23h_ticks.bi5\n",
      "end date good\n",
      "all tries 0\n",
      "BTCUSD\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/00h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/01h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/02h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/03h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/04h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/05h_ticks.bi5\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/06h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/07h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/08h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/09h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/10h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/11h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/12h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/13h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/14h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/15h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/16h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/17h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/18h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/19h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/20h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/21h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/22h_ticks.bi5\n",
      "download failed. continuing..\n",
      "downloading: https://datafeed.dukascopy.com/datafeed/BTCUSD/2021/03/12/23h_ticks.bi5\n",
      "download failed. continuing..\n",
      "all tries 0\n",
      "USA500IDXUSD\n",
      "end date good\n",
      "all tries 0\n",
      "USATECHIDXUSD\n",
      "start date good\n",
      "end date good\n",
      "all tries 0\n"
     ]
    }
   ],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(inst_txt_path, 'r') as f:\n",
    "#     insts = f.read().split('\\n')\n",
    "# print(insts)\n",
    "\n",
    "# for inst in insts:\n",
    "#     print(inst)\n",
    "#     if inst not in os.listdir(path):\n",
    "#         os.mkdir('{}/{}'.format(path, inst))\n",
    "#     print(os.listdir('{}/{}'.format(path, inst)))\n",
    "    \n",
    "#     # consider making some meta-file that keeps last updated date instead of concatting everytime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
